{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a1ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3116\n",
      "Example Input (English): wow !\n",
      "Example Target (Hindi): [START] वाह ! [END]\n",
      "\n",
      "Unique English tokens: 2509\n",
      "Unique Hindi tokens: 3214\n",
      "Max English sentence length: 25\n",
      "Max Hindi sentence length: 29\n",
      "\n",
      "--- Data Shapes ---\n",
      "encoder_input_data shape: (3116, 25)\n",
      "decoder_input_data shape: (3116, 29)\n",
      "decoder_target_data shape: (3116, 29, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # Start with 30, increase to 100+ for better results\n",
    "LATENT_DIM = 256  # Size of the \"context vector\"\n",
    "NUM_SAMPLES = 10000  # Number of sentences to train on\n",
    "DATA_PATH = 'hin.txt'  # <-- This is our manually downloaded file\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 1: Load and Preprocess Data\n",
    "# ----------------------------------------\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[:min(NUM_SAMPLES, len(lines) - 1)]:\n",
    "    # Tab-separated: English[TAB]Hindi[TAB]Attribution\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) < 2:\n",
    "        continue # Skip lines that don't have both English and Hindi\n",
    "\n",
    "    input_text = parts[0]\n",
    "    target_text = parts[1]\n",
    "\n",
    "    # Clean punctuation\n",
    "    input_text = re.sub(r\"([?.!,])\", r\" \\1\", input_text.lower().strip())\n",
    "    target_text = re.sub(r\"([?.!,])\", r\" \\1\", target_text.lower().strip())\n",
    "    \n",
    "    # We add '[START]' and '[END]' tokens to the *target* (Hindi) sentence.\n",
    "    # This tells the model when to start and stop translating.\n",
    "    target_text = f\"[START] {target_text} [END]\"\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "print(f\"Total samples: {len(input_texts)}\")\n",
    "print(f\"Example Input (English): {input_texts[0]}\")\n",
    "print(f\"Example Target (Hindi): {target_texts[0]}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 2: Tokenization (Vectorizing)\n",
    "# ----------------------------------------\n",
    "\n",
    "# We need to convert our words into numbers (indices)\n",
    "# We use separate \"dictionaries\" (Tokenizers) for English and Hindi\n",
    "\n",
    "# Input (English) Tokenizer\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='', oov_token='<unk>' # <unk> = \"unknown word\"\n",
    ")\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "# Target (Hindi) Tokenizer\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='', oov_token='<unk>'\n",
    ")\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "# Get key properties for our model\n",
    "input_word_index = input_tokenizer.word_index\n",
    "target_word_index = target_tokenizer.word_index\n",
    "\n",
    "num_encoder_tokens = len(input_word_index) + 1\n",
    "num_decoder_tokens = len(target_word_index) + 1\n",
    "\n",
    "max_encoder_seq_length = max(len(seq) for seq in input_sequences)\n",
    "max_decoder_seq_length = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "print(f\"\\nUnique English tokens: {num_encoder_tokens}\")\n",
    "print(f\"Unique Hindi tokens: {num_decoder_tokens}\")\n",
    "print(f\"Max English sentence length: {max_encoder_seq_length}\")\n",
    "print(f\"Max Hindi sentence length: {max_decoder_seq_length}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 3: Prepare Data for Training\n",
    "# ----------------------------------------\n",
    "\n",
    "# Pad all sequences to be the same length\n",
    "encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences, maxlen=max_encoder_seq_length, padding='post'\n",
    ")\n",
    "\n",
    "# For the decoder, we need two versions:\n",
    "# 1. decoder_input_data: The target sentence (e.g., \"[START] word1 word2 [END]\")\n",
    "decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences, maxlen=max_decoder_seq_length, padding='post'\n",
    ")\n",
    "\n",
    "# 2. decoder_target_data: The *answer* the model should predict.\n",
    "#    This is the same as decoder_input_data, but \"shifted\" one step.\n",
    "#    e.g., \"word1 word2 [END]\"\n",
    "#    The model sees \"[START]\" and must predict \"word1\".\n",
    "#    The model sees \"word1\" and must predict \"word2\".\n",
    "#\n",
    "# We use 'sparse_categorical_crossentropy' as our loss, so we don't need\n",
    "# to one-hot encode. We just need a 3D array of shape:\n",
    "# (num_samples, max_seq_length, 1) holding the *index* of the correct word.\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(target_sequences), max_decoder_seq_length, 1), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, seq in enumerate(target_sequences):\n",
    "    for t in range(len(seq)):\n",
    "        if t > 0:\n",
    "            # decoder_target_data[i, t-1, 0] will be the index of word `t`\n",
    "            decoder_target_data[i, t - 1, 0] = seq[t]\n",
    "\n",
    "\n",
    "print(f\"\\n--- Data Shapes ---\")\n",
    "print(f\"encoder_input_data shape: {encoder_input_data.shape}\")\n",
    "print(f\"decoder_input_data shape: {decoder_input_data.shape}\")\n",
    "print(f\"decoder_target_data shape: {decoder_target_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdd3d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">642,304</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">822,784</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">825,998</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3214</span>)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m642,304\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m822,784\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m825,998\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m3214\u001b[0m)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,341,710</span> (12.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,341,710\u001b[0m (12.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,341,710</span> (12.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,341,710\u001b[0m (12.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 1/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 170ms/step - accuracy: 0.4789 - loss: 6.6602 - val_accuracy: 0.6185 - val_loss: 6.0052\n",
      "Epoch 2/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.7631 - loss: 5.2860 - val_accuracy: 0.6244 - val_loss: 5.8713\n",
      "Epoch 3/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - accuracy: 0.3585 - loss: 4.9886 - val_accuracy: 0.0765 - val_loss: 5.6506\n",
      "Epoch 4/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - accuracy: 0.0745 - loss: 4.7938 - val_accuracy: 0.0763 - val_loss: 5.6166\n",
      "Epoch 5/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 167ms/step - accuracy: 0.0752 - loss: 4.7053 - val_accuracy: 0.0759 - val_loss: 5.6544\n",
      "Epoch 6/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - accuracy: 0.0749 - loss: 4.6553 - val_accuracy: 0.0760 - val_loss: 5.6226\n",
      "Epoch 7/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 170ms/step - accuracy: 0.0758 - loss: 4.6024 - val_accuracy: 0.0768 - val_loss: 5.5394\n",
      "Epoch 8/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 170ms/step - accuracy: 0.0770 - loss: 4.5436 - val_accuracy: 0.0785 - val_loss: 5.5200\n",
      "Epoch 9/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 178ms/step - accuracy: 0.0805 - loss: 4.4849 - val_accuracy: 0.0804 - val_loss: 5.5007\n",
      "Epoch 10/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 180ms/step - accuracy: 0.0823 - loss: 4.4356 - val_accuracy: 0.0811 - val_loss: 5.4655\n",
      "Epoch 11/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - accuracy: 0.0838 - loss: 4.3900 - val_accuracy: 0.0827 - val_loss: 5.4568\n",
      "Epoch 12/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - accuracy: 0.0849 - loss: 4.3480 - val_accuracy: 0.0815 - val_loss: 5.4390\n",
      "Epoch 13/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 169ms/step - accuracy: 0.0863 - loss: 4.3074 - val_accuracy: 0.0849 - val_loss: 5.4041\n",
      "Epoch 14/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - accuracy: 0.0881 - loss: 4.2636 - val_accuracy: 0.0859 - val_loss: 5.3868\n",
      "Epoch 15/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - accuracy: 0.0898 - loss: 4.2180 - val_accuracy: 0.0887 - val_loss: 5.3567\n",
      "Epoch 16/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 179ms/step - accuracy: 0.0920 - loss: 4.1642 - val_accuracy: 0.0884 - val_loss: 5.3310\n",
      "Epoch 17/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 170ms/step - accuracy: 0.0936 - loss: 4.1097 - val_accuracy: 0.0918 - val_loss: 5.2717\n",
      "Epoch 18/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 178ms/step - accuracy: 0.0949 - loss: 4.0594 - val_accuracy: 0.0943 - val_loss: 5.2283\n",
      "Epoch 19/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 180ms/step - accuracy: 0.0970 - loss: 4.0039 - val_accuracy: 0.0957 - val_loss: 5.2073\n",
      "Epoch 20/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - accuracy: 0.0983 - loss: 3.9555 - val_accuracy: 0.0972 - val_loss: 5.1908\n",
      "Epoch 21/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - accuracy: 0.0996 - loss: 3.9040 - val_accuracy: 0.0969 - val_loss: 5.1741\n",
      "Epoch 22/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 165ms/step - accuracy: 0.1009 - loss: 3.8551 - val_accuracy: 0.0980 - val_loss: 5.1633\n",
      "Epoch 23/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - accuracy: 0.1021 - loss: 3.8070 - val_accuracy: 0.0982 - val_loss: 5.1550\n",
      "Epoch 24/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.1033 - loss: 3.7636 - val_accuracy: 0.0992 - val_loss: 5.1290\n",
      "Epoch 25/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - accuracy: 0.1044 - loss: 3.7165 - val_accuracy: 0.0997 - val_loss: 5.0947\n",
      "Epoch 26/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - accuracy: 0.1048 - loss: 3.6699 - val_accuracy: 0.0990 - val_loss: 5.0918\n",
      "Epoch 27/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - accuracy: 0.1059 - loss: 3.6222 - val_accuracy: 0.1003 - val_loss: 5.0801\n",
      "Epoch 28/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - accuracy: 0.1074 - loss: 3.5767 - val_accuracy: 0.0988 - val_loss: 5.0949\n",
      "Epoch 29/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - accuracy: 0.1082 - loss: 3.5318 - val_accuracy: 0.1024 - val_loss: 5.0595\n",
      "Epoch 30/30\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.1102 - loss: 3.4889 - val_accuracy: 0.1018 - val_loss: 5.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Training Complete ---\n",
      "\n",
      "--- Inference Models Built ---\n",
      "\n",
      "--- Model Testing ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "--------------------------------------------------\n",
      "Input:    the house is haunted .\n",
      "Original: इस घर में भूत है।\n",
      "Predicted: यह बहुत बहुत है।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "--------------------------------------------------\n",
      "Input:    i'm very happy to make your acquaintance .\n",
      "Original: मुझे आपसे मिलकर बहुत खुशी हुई।\n",
      "Predicted: मैं तुम्हें है कि मैं एक हूँ।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "--------------------------------------------------\n",
      "Input:    the children were flying kites .\n",
      "Original: बच्चे पतंग उड़ा रहे थे।\n",
      "Predicted: यह बहुत बहुत बहुत है।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "--------------------------------------------------\n",
      "Input:    tell me the truth .\n",
      "Original: मुझे सच्चाई बताओ।\n",
      "Predicted: मैं एक नहीं हूँ।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "--------------------------------------------------\n",
      "Input:    who knows ?\n",
      "Original: कौन जाने ?\n",
      "Predicted: क्या ?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "--------------------------------------------------\n",
      "Input:    she pulled my shirt .\n",
      "Original: उसने मेरी कमीज़ खींची।\n",
      "Predicted: वह एक बहुत बहुत है।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "--------------------------------------------------\n",
      "Input:    the power went out .\n",
      "Original: बिजली चली गई।\n",
      "Predicted: यह मत !\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "--------------------------------------------------\n",
      "Input:    is your father a teacher ?\n",
      "Original: तेरा बाप टीचर है क्या ?\n",
      "Predicted: क्या क्या क्या क्या ?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "--------------------------------------------------\n",
      "Input:    they must have made a mistake .\n",
      "Original: उनसे ग़लती हो गई होगी।\n",
      "Predicted: हम इस बहुत बहुत बहुत है।\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "--------------------------------------------------\n",
      "Input:    \"where do you live ?\" \"i live in tokyo .\"\n",
      "Original: \"आप कहाँ रहते हैं ?\" \"मैं टोक्यो में रहता हूँ\"\n",
      "Predicted: क्या तुम्हें तुम्हें है कि तुम तो है ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 4: Build the Training Model\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- The Encoder ---\n",
    "# This part reads the English sentence and compresses it into a \"context vector\".\n",
    "\n",
    "# 1. Input layer\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "\n",
    "# 2. Embedding layer: Converts word indices (like 5) into dense vectors (like [0.1, -0.3, ...])\n",
    "enc_embedding_layer = Embedding(num_encoder_tokens, LATENT_DIM, mask_zero=True)\n",
    "enc_emb = enc_embedding_layer(encoder_inputs)\n",
    "\n",
    "# 3. LSTM layer: Processes the sequence.\n",
    "#    We set 'return_state=True' to get the final \"context vector\" (h and c states).\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We only need the states (the context) to pass to the decoder.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# --- The Decoder ---\n",
    "# This part takes the \"context vector\" and generates the Hindi sentence.\n",
    "\n",
    "# 1. Input layer (for the Hindi sentence)\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "\n",
    "# 2. Embedding layer (for Hindi words)\n",
    "dec_embedding_layer = Embedding(num_decoder_tokens, LATENT_DIM, mask_zero=True)\n",
    "dec_emb = dec_embedding_layer(decoder_inputs)\n",
    "\n",
    "# 3. LSTM layer\n",
    "#    We set 'return_sequences=True' to get an output at *every* time step (for every word).\n",
    "#    We give it the 'encoder_states' as its starting point.\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    dec_emb, initial_state=encoder_states # <-- This is the seq2seq magic!\n",
    ")\n",
    "\n",
    "# 4. Output (Dense) layer\n",
    "#    This converts the LSTM's output vector into a probability score for\n",
    "#    *every possible word* in the Hindi vocabulary.\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# --- The Full Training Model ---\n",
    "# Connects the Encoder and Decoder into one model.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy', # Good for integer targets\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 5: Train the Model\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],  # Inputs\n",
    "    decoder_target_data,                       # Target (the \"answers\")\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(\"--- Model Training Complete ---\")\n",
    "model.save('s2s_model.h5')\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 6: Build the Inference (Testing) Models\n",
    "# ----------------------------------------\n",
    "\n",
    "# > **Why do we need new models for testing?**\n",
    "# >\n",
    "# > The `model` we just trained is for **training**. It uses \"Teacher Forcing,\" which means it gets the *entire* correct Hindi sentence as an input all at once.\n",
    "# >\n",
    "# > For **testing** (inference), we don't have the Hindi sentence! We need to generate it *one word at a time*.\n",
    "# >\n",
    "# > We do this by splitting our trained model into two parts:\n",
    "# > 1.  **Encoder Model:** Takes an English sentence and outputs the context vector (states).\n",
    "# > 2.  **Decoder Model:** Takes the context vector *and* the *last predicted word* to predict the *next* word.\n",
    "\n",
    "# 1. The Encoder Model (same as before)\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 2. The Decoder Model (this is the new part)\n",
    "# We need to define new inputs for the states, as they will be fed in a loop\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Get the embedding of the *single* input word\n",
    "dec_emb_inference = dec_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Run the LSTM for *one step* using the previous states\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
    "    dec_emb_inference, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "\n",
    "# Get the word probabilities\n",
    "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "\n",
    "# This is the final decoder model for testing\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs_inf] + decoder_states_inf\n",
    ")\n",
    "\n",
    "print(\"\\n--- Inference Models Built ---\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# ## Step 7: Test the Model (Translate!)\n",
    "# ----------------------------------------\n",
    "\n",
    "# Create reverse lookups to convert indices back to words\n",
    "reverse_input_word_index = {i: word for word, i in input_word_index.items()}\n",
    "reverse_target_word_index = {i: word for word, i in target_word_index.items()}\n",
    "\n",
    "# Get the special [START] and [END] token indices\n",
    "start_token_index = target_word_index['[start]']\n",
    "end_token_index = target_word_index['[end]']\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 1. Encode the input sentence to get the \"context\"\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 2. Start the decoder with just the [START] token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = start_token_index\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "\n",
    "    # 3. Loop to generate words one by one\n",
    "    while not stop_condition:\n",
    "        # 4. Predict the next word and get the new states\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value\n",
    "        )\n",
    "\n",
    "        # 5. Get the word with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index.get(sampled_token_index, '<unk>')\n",
    "\n",
    "        # 6. Check if we should stop\n",
    "        if (sampled_word == '[end]' or\n",
    "            len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "\n",
    "        # 7. Update the inputs for the next loop\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index # The new input is the word we just predicted\n",
    "        states_value = [h, c] # The new state is the state we just got\n",
    "\n",
    "    return \" \".join(decoded_sentence)\n",
    "\n",
    "\n",
    "# --- Let's test it! ---\n",
    "print(\"\\n--- Model Testing ---\")\n",
    "for i in range(10):  # Test on 10 random samples\n",
    "    \n",
    "    # Get a random test sample\n",
    "    test_index = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_input_data[test_index: test_index + 1] # Needs to be batch shape\n",
    "    \n",
    "    original_input = input_texts[test_index]\n",
    "    original_target = target_texts[test_index]\n",
    "    \n",
    "    # Translate\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Input:    {original_input}\")\n",
    "    print(f\"Original: {original_target.replace('[START]', '').replace('[END]', '').strip()}\")\n",
    "    print(f\"Predicted: {decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Above Accuracy Can be increased by various methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
